---
title: "Session 2"
author: "Tim Riffe"
date: "11/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

Today's objective is to get some practice with basic data wrangling. We'll use COVID case data from the Philippines for our example data.

## Read in the data set

Download case data by following the `Download COVID-19 Data drop` link here <https://ncovtracker.doh.gov.ph/>

Stick the file in the `Data` folder (using your usual file browser or whatever). The name of the file is long, sorry, and it contains the date in `yyyymmdd` format, so you'll need to change the path if you download data from a different date.
```{r, message = FALSE}
library(tidyverse)
library(readr)

PH <- read_csv("Data/DOH COVID Data Drop_ 20201110 - 04 Case Information.csv")

dim(PH)
glimpse(PH)
```

There are lots of nice date columns in this file, which we can use for calculating different kinds of lags.

## Calculate times series of cumulative totals

First, let's derive the aggregate series, as typically reported. We take `DateRepConf` as the day data will have likely entered the statistical series. Most places report cumulative sums. Key tricks used here:

1. `n()` counts rows 
2. `group_by()` means that whatever you do after it happens independently within groups.
3. accumulation here happens right within the `ggplot()` call with `cumsum()`

```{r}

Cases <-
  PH %>% 
  group_by(DateRepConf) %>% 
  summarize(N = n())

Cases %>% 
  ggplot(mapping = aes(x = DateRepConf,
                       y = cumsum(N))) +
  geom_line()
```

Now, the same thing for Deaths. First let's see if there are many deaths with unknown dates:

```{r}
# how many deaths with known date of death?
PH %>% 
  filter(!is.na(DateDied)) %>% 
  nrow()

# how many known deaths
PH %>% 
  dplyr::pull(RemovalType) %>% 
  table()
```

There are some deaths, under a percent, where we don't know when it occurred. But that doesn't mean we know nothing. We might know other prior dates, like onset, specimen, date of case registry, and maybe those things vary in a regular way by age or sex or whatever

```{r}
Deaths <-   PH %>% 
  filter(!is.na(DateDied)) %>% 
  group_by(DateDied) %>% 
  dplyr::summarize(N = n())

Deaths %>% 
  ggplot(aes(x = DateDied, y = N)) +
  geom_line()

```

## Calculate reporting lags

How about we calculate the lag distribution between case onset and case registry?
```{r}
PH %>% 
  dplyr::pull(DateOnset) %>% 
  is.na() %>% 
  table()

PH %>% 
  dplyr::pull(DateRepConf) %>% 
  is.na() %>% 
  table()
```

We can only do this for the cases with recorded date of onset (transmission). `DateRepConf` is given for each case, because it's known, necessarily. It's the day the data enter the statistical series. `DateOnset` is known for a bit under half of cases at this writing. If we want to relate onset to reporting, then both dates should be known. Indeed if patterns were stable enough, then maybe it'd make sense to impute onset date using the available covariates? I don't know, worth thinking about. Anyway, moving forward, let's calculate the overall mean lag.
```{r}
PH %>% 
  filter(!is.na(DateOnset)) %>% 
  mutate(CaseReportLag = DateRepConf - DateOnset) %>% 
  summarize(MeanLag = mean(CaseReportLag))
```

Question: Do age and sex matter for the mean lag? Seemingly not that much. Note the use of `%%` (modulo, or *remainder*) in order to group ages to 10-year age groups. A trick worth internalizing.

```{r}
PH %>% 
  filter(!is.na(DateOnset)) %>% 
  mutate(CaseReportLag = DateRepConf - DateOnset,
         Age10 = Age - Age %% 10) %>% 
  group_by(Sex, Age10) %>% 
  summarize(MeanLag = mean(CaseReportLag)) %>% 
  ggplot(aes(x =Age10, y = MeanLag, color = Sex)) + 
  geom_line() + 
  ylim(0,16) # set explicit y limits
```

### Lag distributions

Question: What about the lag *distribution*, of which the mean is just a summary measure, does it vary much? This pipeline is a bit more involved. Step annotation below.

```{r}
library(colorspace)

PH %>% 
  filter(!is.na(DateOnset)) %>% 
  mutate(CaseReportLag = DateRepConf - DateOnset,
         Age10 = Age - Age %% 10) %>% 
  group_by(Sex, Age10, CaseReportLag) %>% 
  summarize(N = n()) %>% 
  group_by(Sex, Age10) %>% 
  filter(sum(N) >= 50) %>% 
  mutate(dist = N / sum(N)) %>% 
  
  # here starts ggplot-land
  ggplot(aes(x = CaseReportLag, 
             y = dist, 
             color = factor(Age10),
             linetype = Sex,
             group = interaction(Sex,Age10))) +
  geom_line() + 
  scale_color_discrete_sequential("ag_GrnYl") +
  xlim(0,50)
  
```

1. cut data down to recorded onset dates using `filter()`
2. calculate lags, and also group ages inside `mutate()`
3. within each observed combination of age group, sex, and lag, count the number of rows with `n()`
4. throw out combinations of age and sex that don't have at least 50 measured lags
5. within age and sex, transform counts into distributions, `dist`
6. overplot the lag distributions. If they overlap nicely then age and sex maybe don't matter much.

It seems that lags only matter some for children? Otherwise, looks like it might be safe to create a single lag distribution *standard* for use in imputing unknown onset dates. We don't actually do that here, but you can perhaps imagine some considerations that might go into it: Are there other observed distributions that might also help achieve the same goal? Would require a bit more exploratory analysis.


## Questions posed in class

### Clarify `n()` vs `sum()` inside tabulation pipelines

This is a demonstration that tabulating microdata to more specific subgroups, and then `sum()`ing those counts is identical to just tabulating that way in the first place `n()`, i.e. in the less specific subgroups. In our case we have one row equal to one observation, but sometimes in surveys you are given population weights, in which case you'd probably rather `sum()` these. Make sense?

```{r}

# A tabulate, then sum within bigger groups
A <- 
   PH %>% 
  filter(!is.na(DateOnset)) %>% 
  mutate(CaseReportLag = DateRepConf - DateOnset,
         Age10 = Age - Age %% 10) %>% 
  group_by(Sex, Age10, CaseReportLag) %>% 
  summarize(N = n()) %>% 
  group_by(Sex, Age10) %>% 
  summarize(N = sum(N))

# B) just tabulate that way in the first place, identical
B <- 
PH %>% 
  filter(!is.na(DateOnset)) %>% 
  mutate(CaseReportLag = DateRepConf - DateOnset,
         Age10 = Age - Age %% 10) %>% 
  group_by(Sex, Age10) %>% 
  summarize(N = n())

```

### `colorspace` package for nice color palettes

A new mini lesson on color palettes, which can be used well or poorly, but this particular tool happens to be structured in a way that promotes making good choices.

1. `colorspace` has a nice selection of base palettes (which you can also customize)
2. there are functions for using these palettes in `ggplot` `color` or `fill` `scales`.

The function name is built out of components:

`scale_(color or fill)_(discrete or continuous)_(qualitative or sequential or diverging)`

Just let autocomplete let you remember how to do it. Then you can just give the palette name you want inside the function. 

```{r}
library(colorspace)
hcl_palettes(plot = TRUE)
scale
# scale_color_discrete_sequential("ag_GrnYl")
```


# Exercise

Is case detection getting faster (better)?

Choose a reasonable cut point for *before-after* (possibly derived from some observation about the overall timeseries of data). 

That is to say, is the lag between onset and *detection* getting shorter, and/or more compact over time. Make it easy and just group all the data before your cutpoint in one group, and all the data after it into another.

Consider separating children vs everyone else, since we saw that it made a different with reporting lags. Here the objective is detection lags rather than reporting lags, so look over the variables and make a reasonable guess at what's needed there.

We will begin the class Wednesday by working through this solution. 

Now the same thing for deaths:


## Possible Solution

```{r}
# Plotting the deads
Deaths %>% 
  ggplot(aes(x=DateDied, 
             y = N)) + 
  geom_line()
```


We can make a zoom into the valley between the 2 peaks:

```{r, echo = FALSE}

# Using lubridate package to transform strings to dates
library(lubridate)

Deaths %>% 
  ggplot(aes(x=DateDied, 
             y = N)) + 
  geom_line() +
  xlim(ymd("2020-03-15"), ymd("2020-07-15"))

```

It looks like May 15th is the lowest point of daily deaths between the 2 peaks in Philippines.

We can try a possible solution by analyzing the distribution of the difference between the `DateOnset` and the `DateResultRelease`.

We will create a flag for "children" vs "no_children" (set at 15 years old) and a flag to indicate the dates before and after May 15th.

Instead of calculating the percentage for each data point, we decided to use the `geom_density` function from `ggplot` to plot density functions. We separated the plots by date using the `facet_grid` function.

We limited the x-axis up to 100 days since the tail of the distribution is too long and does not give much information after that time. But, there is a bump in the tail around 120 days lag which could be interesting to keep analyzing further.

```{r}

  PH %>% 
  filter(!is.na(DateOnset)) %>% 
  mutate(detection.lag = DateResultRelease - DateOnset,
         Age10 = Age - Age %% 10,
        # Creating a variable to flag the before and after May 15th data in the plot
         date_flag = case_when(DateOnset <= ymd("2020-05-15") ~ "01 Before 2020-05-15",
                               TRUE ~ "02 After 2020-05-15"),
        # Creating a variable to flag if the microdata comes from a "child" (here we defined under 15 years old, but can be changed easily)
         children = case_when(Age < 15 ~ "child",
                              TRUE ~ "not_child")
         )  %>%
  # Grouping 
  group_by(DateOnset, Sex, Age10, date_flag, children) %>% 
  summarize(avg.lag = mean(detection.lag)) %>% 
  ggplot(aes(x=avg.lag, color=children)) + 
  # We use the geom_density instead of having to calculate the percentage of each day-lag. Is not exactly the same (kernel vs exact calculation) but good enough for a quick analysis
  geom_density() +
  # Splitting the plots by the feature that shows before and after 15th May
  facet_grid(~ date_flag) +
  # Just to 100 days lag because the tail is too long and doesn't give much information.
  # BUT!: there is a bump for 120 days lag in the time series before 15th May
  # Culd be nice to understand why...
  xlim(0,100)

```

As we can see from the plots, the distributions got narrower after May 15th, indicating that the lag is getting shorter and the process getting better.

At the same time, children's distribution is narrower compared to the non-child after May 15th which suggests that testing for younger ages got better after that date. 

